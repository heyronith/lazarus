{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNRdXmIPoTlAtP6WjqHbszV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/heyronith/lazarus/blob/main/mPox.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import and Set Up"
      ],
      "metadata": {
        "id": "1HP-nA_z8U99"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZOHyxokpF4z",
        "outputId": "6d32be6a-f733-426f-892b-3740297a947a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset extracted successfully.\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Extract the dataset\n",
        "zip_path = '/content/Monkeypox Skin Image Dataset.zip'  # Adjust this path if necessary\n",
        "extract_path = '/content'\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(\"Dataset extracted successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "extract_path = '/content/Monkeypox Skin Image Dataset'\n",
        "print(\"Contents of the extracted folder:\")\n",
        "print(os.listdir(extract_path))\n",
        "\n",
        "# Check if there are any subdirectories\n",
        "for item in os.listdir(extract_path):\n",
        "    item_path = os.path.join(extract_path, item)\n",
        "    if os.path.isdir(item_path):\n",
        "        print(f\"Contents of {item}:\")\n",
        "        print(os.listdir(item_path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVG-6s6irDRK",
        "outputId": "c9184335-7193-4323-c268-da95170c2310"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contents of the extracted folder:\n",
            "['Measles', 'Chickenpox', 'Normal', 'Monkeypox']\n",
            "Contents of Measles:\n",
            "['measles75.png', 'measles5.png', 'measles13.png', 'measles21.png', 'measles42.png', 'measles85.png', 'measles20.png', 'measles37.png', 'measles50.png', 'measles73.png', 'measles63.png', 'measles10.png', 'measles77.png', 'measles16.png', 'measles55.png', 'measles33.png', 'measles60.png', 'measles78.png', 'measles58.png', 'measles67.png', 'measles88.png', 'measles2.png', 'measles32.png', 'measles82.png', 'measles83.png', 'measles28.png', 'measles46.png', 'measles48.png', 'measles24.png', 'measles69.png', 'measles15.png', 'measles4.png', 'measles52.png', 'measles29.png', 'measles80.png', 'measles62.png', 'measles25.png', 'measles68.png', 'measles9.png', 'measles31.png', 'measles26.png', 'measles47.png', 'measles66.png', 'measles19.png', 'measles89.png', 'measles7.png', 'measles49.png', 'measles1.png', 'measles74.png', 'measles81.png', 'measles44.png', 'measles72.png', 'measles11.png', 'measles38.png', 'measles90.png', 'measles34.png', 'measles17.png', 'measles70.png', 'measles8.png', 'measles45.png', 'measles12.png', 'measles59.png', 'measles41.png', 'measles57.png', 'measles87.png', 'measles35.png', 'measles61.png', 'measles27.png', 'measles30.png', 'measles86.png', 'measles23.png', 'measles39.png', 'measles40.png', 'measles22.png', 'measles36.png', 'measles76.png', 'measles18.png', 'measles56.png', 'measles91.png', 'measles79.png', 'measles65.png', 'measles53.png', 'measles43.png', 'measles64.png', 'measles14.png', 'measles84.png', 'measles3.png', 'measles71.png', 'measles51.png', 'measles6.png', 'measles54.png']\n",
            "Contents of Chickenpox:\n",
            "['chickenpox4.png', 'chickenpox18.png', 'chickenpox94.png', 'chickenpox41.png', 'chickenpox53.png', 'chickenpox27.png', 'chickenpox73.png', 'chickenpox85.png', 'chickenpox9.png', 'chickenpox8.png', 'chickenpox89.png', 'chickenpox59.png', 'chickenpox71.png', 'chickenpox95.png', 'chickenpox62.png', 'chickenpox24.png', 'chickenpox86.png', 'chickenpox58.png', 'chickenpox77.png', 'chickenpox88.png', 'chickenpox81.png', 'chickenpox1.png', 'chickenpox3.png', 'chickenpox35.png', 'chickenpox49.png', 'chickenpox25.png', 'chickenpox42.png', 'chickenpox65.png', 'chickenpox68.png', 'chickenpox54.png', 'chickenpox70.png', 'chickenpox37.png', 'chickenpox12.png', 'chickenpox36.png', 'chickenpox2.png', 'chickenpox102.png', 'chickenpox83.png', 'chickenpox10.png', 'chickenpox87.png', 'chickenpox52.png', 'chickenpox97.png', 'chickenpox46.png', 'chickenpox63.png', 'chickenpox28.png', 'chickenpox55.png', 'chickenpox11.png', 'chickenpox7.png', 'chickenpox100.png', 'chickenpox79.png', 'chickenpox14.png', 'chickenpox99.png', 'chickenpox103.png', 'chickenpox20.png', 'chickenpox32.png', 'chickenpox107.png', 'chickenpox40.png', 'chickenpox21.png', 'chickenpox43.png', 'chickenpox64.png', 'chickenpox80.png', 'chickenpox34.png', 'chickenpox90.png', 'chickenpox38.png', 'chickenpox67.png', 'chickenpox13.png', 'chickenpox5.png', 'chickenpox47.png', 'chickenpox78.png', 'chickenpox66.png', 'chickenpox26.png', 'chickenpox44.png', 'chickenpox104.png', 'chickenpox98.png', 'chickenpox57.png', 'chickenpox84.png', 'chickenpox23.png', 'chickenpox72.png', 'chickenpox31.png', 'chickenpox33.png', 'chickenpox22.png', 'chickenpox92.png', 'chickenpox50.png', 'chickenpox15.png', 'chickenpox48.png', 'chickenpox61.png', 'chickenpox6.png', 'chickenpox60.png', 'chickenpox17.png', 'chickenpox91.png', 'chickenpox39.png', 'chickenpox30.png', 'chickenpox45.png', 'chickenpox93.png', 'chickenpox16.png', 'chickenpox82.png', 'chickenpox106.png', 'chickenpox75.png', 'chickenpox76.png', 'chickenpox101.png', 'chickenpox74.png', 'chickenpox56.png', 'chickenpox96.png', 'chickenpox69.png', 'chickenpox51.png', 'chickenpox105.png', 'chickenpox29.png', 'chickenpox19.png']\n",
            "Contents of Normal:\n",
            "['normal200.png', 'normal81.png', 'normal146.png', 'normal6.png', 'normal142.png', 'normal270.png', 'normal151.png', 'normal50.png', 'normal196.png', 'normal113.png', 'normal260.png', 'normal192.png', 'normal42.png', 'normal56.png', 'normal255.png', 'normal201.png', 'normal3.png', 'normal41.png', 'normal61.png', 'normal281.png', 'normal35.png', 'normal59.png', 'normal210.png', 'normal234.png', 'normal156.png', 'normal104.png', 'normal289.png', 'normal130.png', 'normal124.png', 'normal76.png', 'normal92.png', 'normal152.png', 'normal7.png', 'normal1.png', 'normal13.png', 'normal121.png', 'normal99.png', 'normal94.png', 'normal93.png', 'normal84.png', 'normal185.png', 'normal17.png', 'normal176.png', 'normal67.png', 'normal75.png', 'normal63.png', 'normal24.png', 'normal179.png', 'normal206.png', 'normal275.png', 'normal36.png', 'normal287.png', 'normal111.png', 'normal39.png', 'normal126.png', 'normal213.png', 'normal98.png', 'normal55.png', 'normal37.png', 'normal112.png', 'normal108.png', 'normal71.png', 'normal85.png', 'normal68.png', 'normal233.png', 'normal169.png', 'normal139.png', 'normal125.png', 'normal166.png', 'normal231.png', 'normal235.png', 'normal244.png', 'normal64.png', 'normal144.png', 'normal162.png', 'normal123.png', 'normal65.png', 'normal228.png', 'normal102.png', 'normal26.png', 'normal95.png', 'normal227.png', 'normal241.png', 'normal32.png', 'normal283.png', 'normal257.png', 'normal277.png', 'normal149.png', 'normal180.png', 'normal164.png', 'normal240.png', 'normal254.png', 'normal168.png', 'normal143.png', 'normal195.png', 'normal262.png', 'normal199.png', 'normal31.png', 'normal282.png', 'normal148.png', 'normal100.png', 'normal103.png', 'normal181.png', 'normal182.png', 'normal23.png', 'normal9.png', 'normal188.png', 'normal8.png', 'normal137.png', 'normal268.png', 'normal105.png', 'normal253.png', 'normal12.png', 'normal101.png', 'normal47.png', 'normal138.png', 'normal132.png', 'normal28.png', 'normal60.png', 'normal15.png', 'normal280.png', 'normal52.png', 'normal239.png', 'normal34.png', 'normal194.png', 'normal88.png', 'normal175.png', 'normal212.png', 'normal51.png', 'normal220.png', 'normal69.png', 'normal80.png', 'normal120.png', 'normal276.png', 'normal135.png', 'normal247.png', 'normal204.png', 'normal145.png', 'normal218.png', 'normal290.png', 'normal40.png', 'normal202.png', 'normal25.png', 'normal54.png', 'normal46.png', 'normal49.png', 'normal246.png', 'normal97.png', 'normal238.png', 'normal160.png', 'normal5.png', 'normal216.png', 'normal251.png', 'normal207.png', 'normal89.png', 'normal224.png', 'normal245.png', 'normal225.png', 'normal264.png', 'normal107.png', 'normal53.png', 'normal252.png', 'normal20.png', 'normal215.png', 'normal155.png', 'normal154.png', 'normal217.png', 'normal27.png', 'normal147.png', 'normal4.png', 'normal87.png', 'normal273.png', 'normal208.png', 'normal86.png', 'normal172.png', 'normal118.png', 'normal232.png', 'normal209.png', 'normal226.png', 'normal78.png', 'normal18.png', 'normal30.png', 'normal193.png', 'normal250.png', 'normal91.png', 'normal267.png', 'normal116.png', 'normal58.png', 'normal57.png', 'normal237.png', 'normal230.png', 'normal109.png', 'normal174.png', 'normal33.png', 'normal38.png', 'normal219.png', 'normal70.png', 'normal96.png', 'normal114.png', 'normal150.png', 'normal286.png', 'normal214.png', 'normal21.png', 'normal165.png', 'normal243.png', 'normal284.png', 'normal117.png', 'normal141.png', 'normal256.png', 'normal106.png', 'normal191.png', 'normal291.png', 'normal2.png', 'normal279.png', 'normal170.png', 'normal278.png', 'normal205.png', 'normal16.png', 'normal110.png', 'normal292.png', 'normal177.png', 'normal178.png', 'normal82.png', 'normal203.png', 'normal90.png', 'normal222.png', 'normal269.png', 'normal272.png', 'normal173.png', 'normal134.png', 'normal293.png', 'normal44.png', 'normal171.png', 'normal22.png', 'normal186.png', 'normal249.png', 'normal274.png', 'normal153.png', 'normal73.png', 'normal223.png', 'normal163.png', 'normal45.png', 'normal263.png', 'normal236.png', 'normal62.png', 'normal189.png', 'normal187.png', 'normal48.png', 'normal127.png', 'normal29.png', 'normal258.png', 'normal11.png', 'normal77.png', 'normal259.png', 'normal167.png', 'normal10.png', 'normal265.png', 'normal74.png', 'normal131.png', 'normal198.png', 'normal248.png', 'normal261.png', 'normal66.png', 'normal122.png', 'normal115.png', 'normal140.png', 'normal159.png', 'normal83.png', 'normal271.png', 'normal161.png', 'normal190.png', 'normal129.png', 'normal184.png', 'normal197.png', 'normal221.png', 'normal43.png', 'normal285.png', 'normal133.png', 'normal119.png', 'normal158.png', 'normal288.png', 'normal157.png', 'normal14.png', 'normal266.png', 'normal242.png', 'normal72.png', 'normal128.png', 'normal183.png', 'normal229.png', 'normal136.png', 'normal79.png', 'normal19.png', 'normal211.png']\n",
            "Contents of Monkeypox:\n",
            "['monkeypox138.png', 'monkeypox245.png', 'monkeypox169.png', 'monkeypox46.png', 'monkeypox73.png', 'monkeypox71.png', 'monkeypox151.png', 'monkeypox227.png', 'monkeypox155.png', 'monkeypox113.png', 'monkeypox134.png', 'monkeypox62.png', 'monkeypox23.png', 'monkeypox133.png', 'monkeypox203.png', 'monkeypox266.png', 'monkeypox200.png', 'monkeypox39.png', 'monkeypox146.png', 'monkeypox220.png', 'monkeypox246.png', 'monkeypox69.png', 'monkeypox188.png', 'monkeypox119.png', 'monkeypox40.png', 'monkeypox248.png', 'monkeypox250.png', 'monkeypox140.png', 'monkeypox168.png', 'monkeypox126.png', 'monkeypox141.png', 'monkeypox68.png', 'monkeypox78.png', 'monkeypox41.png', 'monkeypox10.png', 'monkeypox48.png', 'monkeypox228.png', 'monkeypox190.png', 'monkeypox242.png', 'monkeypox279.png', 'monkeypox65.png', 'monkeypox6.png', 'monkeypox157.png', 'monkeypox176.png', 'monkeypox129.png', 'monkeypox60.png', 'monkeypox251.png', 'monkeypox11.png', 'monkeypox187.png', 'monkeypox170.png', 'monkeypox106.png', 'monkeypox144.png', 'monkeypox45.png', 'monkeypox247.png', 'monkeypox42.png', 'monkeypox88.png', 'monkeypox243.png', 'monkeypox52.png', 'monkeypox34.png', 'monkeypox212.png', 'monkeypox160.png', 'monkeypox64.png', 'monkeypox17.png', 'monkeypox223.png', 'monkeypox93.png', 'monkeypox205.png', 'monkeypox114.png', 'monkeypox101.png', 'monkeypox36.png', 'monkeypox278.png', 'monkeypox230.png', 'monkeypox107.png', 'monkeypox29.png', 'monkeypox9.png', 'monkeypox184.png', 'monkeypox191.png', 'monkeypox136.png', 'monkeypox208.png', 'monkeypox31.png', 'monkeypox74.png', 'monkeypox257.png', 'monkeypox77.png', 'monkeypox110.png', 'monkeypox207.png', 'monkeypox115.png', 'monkeypox150.png', 'monkeypox276.png', 'monkeypox103.png', 'monkeypox111.png', 'monkeypox268.png', 'monkeypox67.png', 'monkeypox173.png', 'monkeypox179.png', 'monkeypox53.png', 'monkeypox199.png', 'monkeypox262.png', 'monkeypox161.png', 'monkeypox24.png', 'monkeypox273.png', 'monkeypox28.png', 'monkeypox163.png', 'monkeypox256.png', 'monkeypox47.png', 'monkeypox27.png', 'monkeypox148.png', 'monkeypox81.png', 'monkeypox238.png', 'monkeypox38.png', 'monkeypox91.png', 'monkeypox196.png', 'monkeypox4.png', 'monkeypox127.png', 'monkeypox269.png', 'monkeypox49.png', 'monkeypox147.png', 'monkeypox96.png', 'monkeypox51.png', 'monkeypox21.png', 'monkeypox8.png', 'monkeypox15.png', 'monkeypox80.png', 'monkeypox16.png', 'monkeypox97.png', 'monkeypox180.png', 'monkeypox209.png', 'monkeypox253.png', 'monkeypox116.png', 'monkeypox171.png', 'monkeypox83.png', 'monkeypox30.png', 'monkeypox2.png', 'monkeypox128.png', 'monkeypox5.png', 'monkeypox44.png', 'monkeypox118.png', 'monkeypox154.png', 'monkeypox198.png', 'monkeypox56.png', 'monkeypox59.png', 'monkeypox139.png', 'monkeypox54.png', 'monkeypox124.png', 'monkeypox70.png', 'monkeypox224.png', 'monkeypox131.png', 'monkeypox79.png', 'monkeypox86.png', 'monkeypox233.png', 'monkeypox221.png', 'monkeypox259.png', 'monkeypox55.png', 'monkeypox72.png', 'monkeypox66.png', 'monkeypox166.png', 'monkeypox13.png', 'monkeypox197.png', 'monkeypox229.png', 'monkeypox202.png', 'monkeypox263.png', 'monkeypox149.png', 'monkeypox12.png', 'monkeypox234.png', 'monkeypox236.png', 'monkeypox258.png', 'monkeypox201.png', 'monkeypox244.png', 'monkeypox172.png', 'monkeypox123.png', 'monkeypox214.png', 'monkeypox84.png', 'monkeypox50.png', 'monkeypox19.png', 'monkeypox210.png', 'monkeypox213.png', 'monkeypox156.png', 'monkeypox217.png', 'monkeypox87.png', 'monkeypox135.png', 'monkeypox117.png', 'monkeypox26.png', 'monkeypox274.png', 'monkeypox195.png', 'monkeypox63.png', 'monkeypox183.png', 'monkeypox271.png', 'monkeypox152.png', 'monkeypox267.png', 'monkeypox35.png', 'monkeypox153.png', 'monkeypox167.png', 'monkeypox95.png', 'monkeypox120.png', 'monkeypox204.png', 'monkeypox158.png', 'monkeypox215.png', 'monkeypox174.png', 'monkeypox102.png', 'monkeypox178.png', 'monkeypox25.png', 'monkeypox61.png', 'monkeypox185.png', 'monkeypox275.png', 'monkeypox33.png', 'monkeypox222.png', 'monkeypox105.png', 'monkeypox37.png', 'monkeypox255.png', 'monkeypox122.png', 'monkeypox98.png', 'monkeypox125.png', 'monkeypox162.png', 'monkeypox99.png', 'monkeypox32.png', 'monkeypox237.png', 'monkeypox231.png', 'monkeypox240.png', 'monkeypox235.png', 'monkeypox164.png', 'monkeypox239.png', 'monkeypox94.png', 'monkeypox145.png', 'monkeypox121.png', 'monkeypox159.png', 'monkeypox7.png', 'monkeypox192.png', 'monkeypox3.png', 'monkeypox137.png', 'monkeypox254.png', 'monkeypox165.png', 'monkeypox90.png', 'monkeypox218.png', 'monkeypox272.png', 'monkeypox142.png', 'monkeypox1.png', 'monkeypox225.png', 'monkeypox112.png', 'monkeypox109.png', 'monkeypox58.png', 'monkeypox252.png', 'monkeypox261.png', 'monkeypox130.png', 'monkeypox265.png', 'monkeypox76.png', 'monkeypox89.png', 'monkeypox193.png', 'monkeypox241.png', 'monkeypox14.png', 'monkeypox20.png', 'monkeypox232.png', 'monkeypox182.png', 'monkeypox216.png', 'monkeypox92.png', 'monkeypox219.png', 'monkeypox177.png', 'monkeypox132.png', 'monkeypox189.png', 'monkeypox186.png', 'monkeypox82.png', 'monkeypox85.png', 'monkeypox108.png', 'monkeypox143.png', 'monkeypox264.png', 'monkeypox18.png', 'monkeypox211.png', 'monkeypox100.png', 'monkeypox260.png', 'monkeypox194.png', 'monkeypox270.png', 'monkeypox249.png', 'monkeypox22.png', 'monkeypox75.png', 'monkeypox57.png', 'monkeypox104.png', 'monkeypox277.png', 'monkeypox175.png', 'monkeypox181.png', 'monkeypox43.png', 'monkeypox206.png', 'monkeypox226.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import albumentations as A\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "def create_dataset_df(data_dir):\n",
        "    image_paths = []\n",
        "    labels = []\n",
        "\n",
        "    for class_folder in os.listdir(data_dir):\n",
        "        class_path = os.path.join(data_dir, class_folder)\n",
        "        if os.path.isdir(class_path):\n",
        "            for image_file in os.listdir(class_path):\n",
        "                if image_file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                    image_paths.append(os.path.join(class_folder, image_file))\n",
        "                    labels.append(class_folder)\n",
        "\n",
        "    df = pd.DataFrame({'image_path': image_paths, 'label': labels})\n",
        "    return df\n",
        "\n",
        "# Create the original dataset DataFrame\n",
        "extract_path = '/content/Monkeypox Skin Image Dataset'\n",
        "df_original = create_dataset_df(extract_path)\n",
        "\n",
        "print(\"Original dataset created.\")\n",
        "print(f\"Dataset size: {len(df_original)}\")\n",
        "print(\"\\nClass distribution:\")\n",
        "print(df_original['label'].value_counts())\n",
        "\n",
        "# Split the data into train and test sets\n",
        "train_df, test_df = train_test_split(df_original, test_size=0.2, stratify=df_original['label'], random_state=42)\n",
        "\n",
        "print(\"\\nTraining set size:\", len(train_df))\n",
        "print(\"Test set size:\", len(test_df))\n",
        "\n",
        "# Create augmented dataset\n",
        "def get_train_augmentations(height=224, width=224):\n",
        "    return A.Compose([\n",
        "        A.RandomRotate90(p=0.5),\n",
        "        A.Rotate(limit=45, p=0.5),\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.VerticalFlip(p=0.5),\n",
        "        A.RandomScale(scale_limit=(0.8, 1.25), p=0.5),\n",
        "        A.ShiftScaleRotate(shift_limit=0.3, scale_limit=0.3, rotate_limit=45, p=0.5),\n",
        "        A.RandomBrightnessContrast(brightness_limit=(0.1, 2), contrast_limit=(0.5, 2), p=0.5),\n",
        "        A.HueSaturationValue(hue_shift_limit=0.5, sat_shift_limit=(0.2, 3), val_shift_limit=0, p=0.5),\n",
        "        A.Resize(height=height, width=width),\n",
        "    ])\n",
        "\n",
        "def augment_image(image_path, augmentations):\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    image = np.array(image)\n",
        "    augmented = augmentations(image=image)\n",
        "    return augmented['image']\n",
        "\n",
        "augmentations = get_train_augmentations()\n",
        "augmented_images = []\n",
        "augmented_labels = []\n",
        "\n",
        "for _, row in train_df.iterrows():\n",
        "    img_path = os.path.join(extract_path, row['image_path'])\n",
        "    label = row['label']\n",
        "\n",
        "    # Add original image\n",
        "    augmented_images.append(img_path)\n",
        "    augmented_labels.append(label)\n",
        "\n",
        "    # Add augmented images\n",
        "    for _ in range(11):  # Create 11 augmented versions of each image\n",
        "        aug_img = augment_image(img_path, augmentations)\n",
        "        augmented_images.append(aug_img)\n",
        "        augmented_labels.append(label)\n",
        "\n",
        "# Create augmented dataset DataFrame\n",
        "df_augmented = pd.DataFrame({'image_path': augmented_images, 'label': augmented_labels})\n",
        "\n",
        "print(\"\\nAugmented dataset created.\")\n",
        "print(f\"Augmented dataset size: {len(df_augmented)}\")\n",
        "print(\"\\nAugmented class distribution:\")\n",
        "print(df_augmented['label'].value_counts())\n",
        "\n",
        "# Save the datasets\n",
        "train_df.to_csv('train_original.csv', index=False)\n",
        "test_df.to_csv('test_original.csv', index=False)\n",
        "df_augmented.to_csv('train_augmented.csv', index=False)\n",
        "\n",
        "print(\"\\nDatasets saved as CSV files.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9gWzyxApHWg",
        "outputId": "a5b21b8c-1e9c-419b-ce17-7e051611a020"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original dataset created.\n",
            "Dataset size: 770\n",
            "\n",
            "Class distribution:\n",
            "label\n",
            "Normal        293\n",
            "Monkeypox     279\n",
            "Chickenpox    107\n",
            "Measles        91\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Training set size: 616\n",
            "Test set size: 154\n",
            "\n",
            "Augmented dataset created.\n",
            "Augmented dataset size: 7392\n",
            "\n",
            "Augmented class distribution:\n",
            "label\n",
            "Normal        2808\n",
            "Monkeypox     2676\n",
            "Chickenpox    1032\n",
            "Measles        876\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Datasets saved as CSV files.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase1: Training on Original Dataset"
      ],
      "metadata": {
        "id": "khO1GOiL79Ne"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Custom Model Definition\n",
        "class SEBlock(nn.Module):\n",
        "    def __init__(self, channel, reduction=16):\n",
        "        super(SEBlock, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(channel, channel // reduction, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(channel // reduction, channel, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        y = self.avg_pool(x).view(b, c)\n",
        "        y = self.fc(y).view(b, c, 1, 1)\n",
        "        return x * y.expand_as(x)\n",
        "\n",
        "class FPN(nn.Module):\n",
        "    def __init__(self, in_channels_list, out_channels):\n",
        "        super(FPN, self).__init__()\n",
        "        self.lateral_convs = nn.ModuleList([\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "            for in_channels in in_channels_list\n",
        "        ])\n",
        "        self.fpn_convs = nn.ModuleList([\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
        "            for _ in in_channels_list\n",
        "        ])\n",
        "\n",
        "    def forward(self, features):\n",
        "        last_feature = self.lateral_convs[-1](features[-1])\n",
        "        fpn_features = [self.fpn_convs[-1](last_feature)]\n",
        "\n",
        "        for feature, lateral_conv, fpn_conv in zip(\n",
        "            features[-2::-1], self.lateral_convs[-2::-1], self.fpn_convs[-2::-1]\n",
        "        ):\n",
        "            lateral = lateral_conv(feature)\n",
        "            feat_shape = lateral.shape[-2:]\n",
        "            top_down = F.interpolate(last_feature, size=feat_shape, mode='nearest')\n",
        "            last_feature = lateral + top_down\n",
        "            fpn_features.append(fpn_conv(last_feature))\n",
        "\n",
        "        return fpn_features[::-1]\n",
        "\n",
        "class CustomModel(nn.Module):\n",
        "    def __init__(self, num_classes=4):\n",
        "        super(CustomModel, self).__init__()\n",
        "        self.efficientnet = models.efficientnet_v2_s(pretrained=True)\n",
        "\n",
        "        # Get the number of features from the last layer of EfficientNetV2\n",
        "        num_features = self.efficientnet.classifier[1].in_features\n",
        "\n",
        "        # Remove the classifier\n",
        "        self.efficientnet = nn.Sequential(*list(self.efficientnet.children())[:-1])\n",
        "\n",
        "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc1 = nn.Linear(num_features, 512)\n",
        "        self.dropout1 = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.dropout2 = nn.Dropout(0.3)\n",
        "        self.fc3 = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.efficientnet(x)\n",
        "        x = self.gap(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Initialize the model\n",
        "model = CustomModel()\n",
        "print(f\"Model initialized with {sum(p.numel() for p in model.parameters())} parameters\")\n",
        "\n",
        "# Move model to device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Rest of the code remains the same...\n",
        "\n",
        "# Custom Dataset\n",
        "class MonkeypoxDataset(Dataset):\n",
        "    def __init__(self, csv_file, root_dir, transform=None):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.class_to_idx = {'Normal': 0, 'Monkeypox': 1, 'Chickenpox': 2, 'Measles': 3}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.root_dir, self.data.iloc[idx, 0])\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        label = self.class_to_idx[self.data.iloc[idx, 1]]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# Data transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Create datasets and dataloaders\n",
        "train_dataset = MonkeypoxDataset('train_original.csv', '/content/Monkeypox Skin Image Dataset', transform=transform)\n",
        "test_dataset = MonkeypoxDataset('test_original.csv', '/content/Monkeypox Skin Image Dataset', transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "# Initialize the model\n",
        "model = CustomModel()\n",
        "\n",
        "# Move model to device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Loss function (Focal Loss)\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=1, gamma=2):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        focal_loss = self.alpha * (1-pt)**self.gamma * ce_loss\n",
        "        return focal_loss.mean()\n",
        "\n",
        "criterion = FocalLoss()\n",
        "\n",
        "# Optimizer and learning rate scheduler\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
        "\n",
        "# Training function\n",
        "def train(model, train_loader, criterion, optimizer, scheduler, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "        if i % 10 == 0:  # Print every 10 mini-batches\n",
        "            print(f'Batch {i}, Loss: {loss.item():.4f}')\n",
        "\n",
        "    scheduler.step()\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    return epoch_loss\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, test_loader, device):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
        "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
        "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "\n",
        "    # For AUC, we need probability scores\n",
        "    all_probs = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, _ in test_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            outputs = model(inputs)\n",
        "            probs = F.softmax(outputs, dim=1)\n",
        "            all_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "    all_probs = np.array(all_probs)\n",
        "    auc = roc_auc_score(all_labels, all_probs, multi_class='ovr', average='weighted')\n",
        "\n",
        "    return accuracy, precision, recall, f1, auc\n",
        "\n",
        "# Training loop\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "num_epochs = 100\n",
        "best_accuracy = 0.0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    train_loss = train(model, train_loader, criterion, optimizer, scheduler, device)\n",
        "    accuracy, precision, recall, f1, auc = evaluate(model, test_loader, device)\n",
        "\n",
        "    print(f\"Train Loss: {train_loss:.4f}\")\n",
        "    print(f\"Test Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}, AUC: {auc:.4f}\")\n",
        "\n",
        "    if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "\n",
        "print(\"Training completed.\")\n",
        "\n",
        "# Load the best model and evaluate\n",
        "model.load_state_dict(torch.load('best_model.pth'))\n",
        "final_accuracy, final_precision, final_recall, final_f1, final_auc = evaluate(model, test_loader, device)\n",
        "\n",
        "print(\"Final Results on Original Dataset:\")\n",
        "print(f\"Accuracy: {final_accuracy:.4f}\")\n",
        "print(f\"Precision: {final_precision:.4f}\")\n",
        "print(f\"Recall: {final_recall:.4f}\")\n",
        "print(f\"F1-score: {final_f1:.4f}\")\n",
        "print(f\"AUC: {final_auc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "-LYHiPM0pHbt",
        "outputId": "cc47d8a1-d3c5-43ef-9f17-b5307051e03e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_V2_S_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_V2_S_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/efficientnet_v2_s-dd5fe13b.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_v2_s-dd5fe13b.pth\n",
            "100%|██████████| 82.7M/82.7M [00:00<00:00, 125MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model initialized with 20965716 parameters\n",
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_V2_S_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_V2_S_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Epoch 1/100\n",
            "Batch 0, Loss: 0.7811\n",
            "Batch 10, Loss: 0.5882\n",
            "Train Loss: 0.5290\n",
            "Test Accuracy: 0.6494, Precision: 0.7196, Recall: 0.6494, F1: 0.6579, AUC: 0.8610\n",
            "Epoch 2/100\n",
            "Batch 0, Loss: 0.2818\n",
            "Batch 10, Loss: 0.4154\n",
            "Train Loss: 0.2939\n",
            "Test Accuracy: 0.8506, Precision: 0.8544, Recall: 0.8506, F1: 0.8434, AUC: 0.9685\n",
            "Epoch 3/100\n",
            "Batch 0, Loss: 0.0934\n",
            "Batch 10, Loss: 0.2270\n",
            "Train Loss: 0.2276\n",
            "Test Accuracy: 0.8052, Precision: 0.8430, Recall: 0.8052, F1: 0.8000, AUC: 0.9708\n",
            "Epoch 4/100\n",
            "Batch 0, Loss: 0.1624\n",
            "Batch 10, Loss: 0.0722\n",
            "Train Loss: 0.1489\n",
            "Test Accuracy: 0.9351, Precision: 0.9353, Recall: 0.9351, F1: 0.9346, AUC: 0.9748\n",
            "Epoch 5/100\n",
            "Batch 0, Loss: 0.0094\n",
            "Batch 10, Loss: 0.0689\n",
            "Train Loss: 0.1452\n",
            "Test Accuracy: 0.8701, Precision: 0.9118, Recall: 0.8701, F1: 0.8784, AUC: 0.9841\n",
            "Epoch 6/100\n",
            "Batch 0, Loss: 0.0751\n",
            "Batch 10, Loss: 0.0960\n",
            "Train Loss: 0.0669\n",
            "Test Accuracy: 0.9221, Precision: 0.9276, Recall: 0.9221, F1: 0.9214, AUC: 0.9906\n",
            "Epoch 7/100\n",
            "Batch 0, Loss: 0.0284\n",
            "Batch 10, Loss: 0.0038\n",
            "Train Loss: 0.0437\n",
            "Test Accuracy: 0.8961, Precision: 0.9038, Recall: 0.8961, F1: 0.8984, AUC: 0.9895\n",
            "Epoch 8/100\n",
            "Batch 0, Loss: 0.0049\n",
            "Batch 10, Loss: 0.0069\n",
            "Train Loss: 0.0287\n",
            "Test Accuracy: 0.9351, Precision: 0.9395, Recall: 0.9351, F1: 0.9351, AUC: 0.9930\n",
            "Epoch 9/100\n",
            "Batch 0, Loss: 0.0297\n",
            "Batch 10, Loss: 0.0021\n",
            "Train Loss: 0.0271\n",
            "Test Accuracy: 0.9221, Precision: 0.9309, Recall: 0.9221, F1: 0.9241, AUC: 0.9916\n",
            "Epoch 10/100\n",
            "Batch 0, Loss: 0.0270\n",
            "Batch 10, Loss: 0.0084\n",
            "Train Loss: 0.0077\n",
            "Test Accuracy: 0.9351, Precision: 0.9398, Recall: 0.9351, F1: 0.9358, AUC: 0.9916\n",
            "Epoch 11/100\n",
            "Batch 0, Loss: 0.0032\n",
            "Batch 10, Loss: 0.0021\n",
            "Train Loss: 0.0358\n",
            "Test Accuracy: 0.8571, Precision: 0.8766, Recall: 0.8571, F1: 0.8481, AUC: 0.9797\n",
            "Epoch 12/100\n",
            "Batch 0, Loss: 0.0095\n",
            "Batch 10, Loss: 0.0200\n",
            "Train Loss: 0.1718\n",
            "Test Accuracy: 0.8182, Precision: 0.8252, Recall: 0.8182, F1: 0.8157, AUC: 0.9248\n",
            "Epoch 13/100\n",
            "Batch 0, Loss: 0.1263\n",
            "Batch 10, Loss: 0.0532\n",
            "Train Loss: 0.1112\n",
            "Test Accuracy: 0.7013, Precision: 0.8217, Recall: 0.7013, F1: 0.7251, AUC: 0.9203\n",
            "Epoch 14/100\n",
            "Batch 0, Loss: 0.0141\n",
            "Batch 10, Loss: 0.1803\n",
            "Train Loss: 0.1272\n",
            "Test Accuracy: 0.8182, Precision: 0.8184, Recall: 0.8182, F1: 0.8174, AUC: 0.9484\n",
            "Epoch 15/100\n",
            "Batch 0, Loss: 0.0231\n",
            "Batch 10, Loss: 0.1356\n",
            "Train Loss: 0.1290\n",
            "Test Accuracy: 0.8377, Precision: 0.8675, Recall: 0.8377, F1: 0.8333, AUC: 0.9451\n",
            "Epoch 16/100\n",
            "Batch 0, Loss: 0.0623\n",
            "Batch 10, Loss: 0.0468\n",
            "Train Loss: 0.1043\n",
            "Test Accuracy: 0.8896, Precision: 0.8976, Recall: 0.8896, F1: 0.8848, AUC: 0.9744\n",
            "Epoch 17/100\n",
            "Batch 0, Loss: 0.0285\n",
            "Batch 10, Loss: 0.1340\n",
            "Train Loss: 0.0506\n",
            "Test Accuracy: 0.8636, Precision: 0.9067, Recall: 0.8636, F1: 0.8713, AUC: 0.9712\n",
            "Epoch 18/100\n",
            "Batch 0, Loss: 0.1449\n",
            "Batch 10, Loss: 0.0075\n",
            "Train Loss: 0.1065\n",
            "Test Accuracy: 0.8442, Precision: 0.8715, Recall: 0.8442, F1: 0.8525, AUC: 0.9798\n",
            "Epoch 19/100\n",
            "Batch 0, Loss: 0.0161\n",
            "Batch 10, Loss: 0.0084\n",
            "Train Loss: 0.0449\n",
            "Test Accuracy: 0.8896, Precision: 0.8975, Recall: 0.8896, F1: 0.8887, AUC: 0.9791\n",
            "Epoch 20/100\n",
            "Batch 0, Loss: 0.0076\n",
            "Batch 10, Loss: 0.0010\n",
            "Train Loss: 0.0251\n",
            "Test Accuracy: 0.8766, Precision: 0.8906, Recall: 0.8766, F1: 0.8757, AUC: 0.9846\n",
            "Epoch 21/100\n",
            "Batch 0, Loss: 0.0021\n",
            "Batch 10, Loss: 0.0143\n",
            "Train Loss: 0.0236\n",
            "Test Accuracy: 0.8636, Precision: 0.8646, Recall: 0.8636, F1: 0.8634, AUC: 0.9787\n",
            "Epoch 22/100\n",
            "Batch 0, Loss: 0.0456\n",
            "Batch 10, Loss: 0.0020\n",
            "Train Loss: 0.0188\n",
            "Test Accuracy: 0.9156, Precision: 0.9153, Recall: 0.9156, F1: 0.9152, AUC: 0.9741\n",
            "Epoch 23/100\n",
            "Batch 0, Loss: 0.0161\n",
            "Batch 10, Loss: 0.0437\n",
            "Train Loss: 0.0065\n",
            "Test Accuracy: 0.9286, Precision: 0.9299, Recall: 0.9286, F1: 0.9286, AUC: 0.9793\n",
            "Epoch 24/100\n",
            "Batch 0, Loss: 0.0003\n",
            "Batch 10, Loss: 0.0001\n",
            "Train Loss: 0.0033\n",
            "Test Accuracy: 0.9026, Precision: 0.9028, Recall: 0.9026, F1: 0.9001, AUC: 0.9752\n",
            "Epoch 25/100\n",
            "Batch 0, Loss: 0.0013\n",
            "Batch 10, Loss: 0.0112\n",
            "Train Loss: 0.0030\n",
            "Test Accuracy: 0.9026, Precision: 0.9037, Recall: 0.9026, F1: 0.9027, AUC: 0.9786\n",
            "Epoch 26/100\n",
            "Batch 0, Loss: 0.0000\n",
            "Batch 10, Loss: 0.0099\n",
            "Train Loss: 0.0120\n",
            "Test Accuracy: 0.9091, Precision: 0.9098, Recall: 0.9091, F1: 0.9085, AUC: 0.9807\n",
            "Epoch 27/100\n",
            "Batch 0, Loss: 0.0000\n",
            "Batch 10, Loss: 0.0003\n",
            "Train Loss: 0.0006\n",
            "Test Accuracy: 0.9091, Precision: 0.9098, Recall: 0.9091, F1: 0.9085, AUC: 0.9812\n",
            "Epoch 28/100\n",
            "Batch 0, Loss: 0.0040\n",
            "Batch 10, Loss: 0.0008\n",
            "Train Loss: 0.0008\n",
            "Test Accuracy: 0.9221, Precision: 0.9236, Recall: 0.9221, F1: 0.9223, AUC: 0.9803\n",
            "Epoch 29/100\n",
            "Batch 0, Loss: 0.0002\n",
            "Batch 10, Loss: 0.0001\n",
            "Train Loss: 0.0172\n",
            "Test Accuracy: 0.9091, Precision: 0.9123, Recall: 0.9091, F1: 0.9096, AUC: 0.9818\n",
            "Epoch 30/100\n",
            "Batch 0, Loss: 0.0009\n",
            "Batch 10, Loss: 0.0031\n",
            "Train Loss: 0.0044\n",
            "Test Accuracy: 0.9091, Precision: 0.9125, Recall: 0.9091, F1: 0.9096, AUC: 0.9818\n",
            "Epoch 31/100\n",
            "Batch 0, Loss: 0.0034\n",
            "Batch 10, Loss: 0.0002\n",
            "Train Loss: 0.0245\n",
            "Test Accuracy: 0.8506, Precision: 0.8550, Recall: 0.8506, F1: 0.8516, AUC: 0.9606\n",
            "Epoch 32/100\n",
            "Batch 0, Loss: 0.0010\n",
            "Batch 10, Loss: 0.0025\n",
            "Train Loss: 0.0575\n",
            "Test Accuracy: 0.8831, Precision: 0.8841, Recall: 0.8831, F1: 0.8823, AUC: 0.9765\n",
            "Epoch 33/100\n",
            "Batch 0, Loss: 0.3121\n",
            "Batch 10, Loss: 0.0806\n",
            "Train Loss: 0.0967\n",
            "Test Accuracy: 0.8377, Precision: 0.8501, Recall: 0.8377, F1: 0.8388, AUC: 0.9633\n",
            "Epoch 34/100\n",
            "Batch 0, Loss: 0.0502\n",
            "Batch 10, Loss: 0.2159\n",
            "Train Loss: 0.0708\n",
            "Test Accuracy: 0.7597, Precision: 0.8196, Recall: 0.7597, F1: 0.7542, AUC: 0.9091\n",
            "Epoch 35/100\n",
            "Batch 0, Loss: 0.0395\n",
            "Batch 10, Loss: 0.0213\n",
            "Train Loss: 0.1269\n",
            "Test Accuracy: 0.8442, Precision: 0.8580, Recall: 0.8442, F1: 0.8366, AUC: 0.9739\n",
            "Epoch 36/100\n",
            "Batch 0, Loss: 0.0642\n",
            "Batch 10, Loss: 0.0291\n",
            "Train Loss: 0.1088\n",
            "Test Accuracy: 0.6558, Precision: 0.7877, Recall: 0.6558, F1: 0.6242, AUC: 0.9211\n",
            "Epoch 37/100\n",
            "Batch 0, Loss: 0.0220\n",
            "Batch 10, Loss: 0.0577\n",
            "Train Loss: 0.0647\n",
            "Test Accuracy: 0.7857, Precision: 0.8322, Recall: 0.7857, F1: 0.7750, AUC: 0.9691\n",
            "Epoch 38/100\n",
            "Batch 0, Loss: 0.0008\n",
            "Batch 10, Loss: 0.0017\n",
            "Train Loss: 0.0895\n",
            "Test Accuracy: 0.8961, Precision: 0.8988, Recall: 0.8961, F1: 0.8951, AUC: 0.9827\n",
            "Epoch 39/100\n",
            "Batch 0, Loss: 0.0041\n",
            "Batch 10, Loss: 0.0094\n",
            "Train Loss: 0.0503\n",
            "Test Accuracy: 0.8442, Precision: 0.8778, Recall: 0.8442, F1: 0.8339, AUC: 0.9691\n",
            "Epoch 40/100\n",
            "Batch 0, Loss: 0.0191\n",
            "Batch 10, Loss: 0.0579\n",
            "Train Loss: 0.0133\n",
            "Test Accuracy: 0.9091, Precision: 0.9089, Recall: 0.9091, F1: 0.9089, AUC: 0.9833\n",
            "Epoch 41/100\n",
            "Batch 0, Loss: 0.0007\n",
            "Batch 10, Loss: 0.0000\n",
            "Train Loss: 0.0181\n",
            "Test Accuracy: 0.8442, Precision: 0.8543, Recall: 0.8442, F1: 0.8475, AUC: 0.9546\n",
            "Epoch 42/100\n",
            "Batch 0, Loss: 0.0010\n",
            "Batch 10, Loss: 0.0001\n",
            "Train Loss: 0.0512\n",
            "Test Accuracy: 0.8701, Precision: 0.8747, Recall: 0.8701, F1: 0.8680, AUC: 0.9723\n",
            "Epoch 43/100\n",
            "Batch 0, Loss: 0.0034\n",
            "Batch 10, Loss: 0.0075\n",
            "Train Loss: 0.0196\n",
            "Test Accuracy: 0.8636, Precision: 0.8659, Recall: 0.8636, F1: 0.8624, AUC: 0.9817\n",
            "Epoch 44/100\n",
            "Batch 0, Loss: 0.0750\n",
            "Batch 10, Loss: 0.0032\n",
            "Train Loss: 0.0254\n",
            "Test Accuracy: 0.8961, Precision: 0.8953, Recall: 0.8961, F1: 0.8947, AUC: 0.9849\n",
            "Epoch 45/100\n",
            "Batch 0, Loss: 0.0003\n",
            "Batch 10, Loss: 0.0007\n",
            "Train Loss: 0.0197\n",
            "Test Accuracy: 0.8831, Precision: 0.8895, Recall: 0.8831, F1: 0.8847, AUC: 0.9796\n",
            "Epoch 46/100\n",
            "Batch 0, Loss: 0.0012\n",
            "Batch 10, Loss: 0.0245\n",
            "Train Loss: 0.0229\n",
            "Test Accuracy: 0.8896, Precision: 0.9030, Recall: 0.8896, F1: 0.8899, AUC: 0.9830\n",
            "Epoch 47/100\n",
            "Batch 0, Loss: 0.0044\n",
            "Batch 10, Loss: 0.0352\n",
            "Train Loss: 0.0190\n",
            "Test Accuracy: 0.9156, Precision: 0.9177, Recall: 0.9156, F1: 0.9164, AUC: 0.9893\n",
            "Epoch 48/100\n",
            "Batch 0, Loss: 0.0012\n",
            "Batch 10, Loss: 0.0198\n",
            "Train Loss: 0.0101\n",
            "Test Accuracy: 0.8636, Precision: 0.8848, Recall: 0.8636, F1: 0.8682, AUC: 0.9838\n",
            "Epoch 49/100\n",
            "Batch 0, Loss: 0.0000\n",
            "Batch 10, Loss: 0.0001\n",
            "Train Loss: 0.0146\n",
            "Test Accuracy: 0.8896, Precision: 0.8921, Recall: 0.8896, F1: 0.8889, AUC: 0.9844\n",
            "Epoch 50/100\n",
            "Batch 0, Loss: 0.0056\n",
            "Batch 10, Loss: 0.0019\n",
            "Train Loss: 0.0094\n",
            "Test Accuracy: 0.9091, Precision: 0.9238, Recall: 0.9091, F1: 0.9138, AUC: 0.9836\n",
            "Epoch 51/100\n",
            "Batch 0, Loss: 0.0280\n",
            "Batch 10, Loss: 0.0007\n",
            "Train Loss: 0.0091\n",
            "Test Accuracy: 0.9091, Precision: 0.9116, Recall: 0.9091, F1: 0.9100, AUC: 0.9874\n",
            "Epoch 52/100\n",
            "Batch 0, Loss: 0.0000\n",
            "Batch 10, Loss: 0.0000\n",
            "Train Loss: 0.0088\n",
            "Test Accuracy: 0.9091, Precision: 0.9117, Recall: 0.9091, F1: 0.9098, AUC: 0.9914\n",
            "Epoch 53/100\n",
            "Batch 0, Loss: 0.0000\n",
            "Batch 10, Loss: 0.0005\n",
            "Train Loss: 0.0092\n",
            "Test Accuracy: 0.9221, Precision: 0.9276, Recall: 0.9221, F1: 0.9213, AUC: 0.9925\n",
            "Epoch 54/100\n",
            "Batch 0, Loss: 0.0009\n",
            "Batch 10, Loss: 0.0001\n",
            "Train Loss: 0.0194\n",
            "Test Accuracy: 0.9026, Precision: 0.9085, Recall: 0.9026, F1: 0.9024, AUC: 0.9923\n",
            "Epoch 55/100\n",
            "Batch 0, Loss: 0.0000\n",
            "Batch 10, Loss: 0.0005\n",
            "Train Loss: 0.0019\n",
            "Test Accuracy: 0.9221, Precision: 0.9247, Recall: 0.9221, F1: 0.9216, AUC: 0.9915\n",
            "Epoch 56/100\n",
            "Batch 0, Loss: 0.0016\n",
            "Batch 10, Loss: 0.0002\n",
            "Train Loss: 0.0009\n",
            "Test Accuracy: 0.9221, Precision: 0.9315, Recall: 0.9221, F1: 0.9232, AUC: 0.9912\n",
            "Epoch 57/100\n",
            "Batch 0, Loss: 0.0000\n",
            "Batch 10, Loss: 0.0000\n",
            "Train Loss: 0.0012\n",
            "Test Accuracy: 0.9286, Precision: 0.9331, Recall: 0.9286, F1: 0.9287, AUC: 0.9917\n",
            "Epoch 58/100\n",
            "Batch 0, Loss: 0.0003\n",
            "Batch 10, Loss: 0.0000\n",
            "Train Loss: 0.0003\n",
            "Test Accuracy: 0.9156, Precision: 0.9169, Recall: 0.9156, F1: 0.9152, AUC: 0.9891\n",
            "Epoch 59/100\n",
            "Batch 0, Loss: 0.0000\n",
            "Batch 10, Loss: 0.0001\n",
            "Train Loss: 0.0026\n",
            "Test Accuracy: 0.9091, Precision: 0.9106, Recall: 0.9091, F1: 0.9085, AUC: 0.9900\n",
            "Epoch 60/100\n",
            "Batch 0, Loss: 0.0001\n",
            "Batch 10, Loss: 0.0000\n",
            "Train Loss: 0.0065\n",
            "Test Accuracy: 0.9156, Precision: 0.9156, Recall: 0.9156, F1: 0.9151, AUC: 0.9903\n",
            "Epoch 61/100\n",
            "Batch 0, Loss: 0.0000\n",
            "Batch 10, Loss: 0.0000\n",
            "Train Loss: 0.0003\n",
            "Test Accuracy: 0.9221, Precision: 0.9228, Recall: 0.9221, F1: 0.9215, AUC: 0.9904\n",
            "Epoch 62/100\n",
            "Batch 0, Loss: 0.0000\n",
            "Batch 10, Loss: 0.0000\n",
            "Train Loss: 0.0009\n",
            "Test Accuracy: 0.9091, Precision: 0.9115, Recall: 0.9091, F1: 0.9089, AUC: 0.9908\n",
            "Epoch 63/100\n",
            "Batch 0, Loss: 0.0000\n",
            "Batch 10, Loss: 0.0000\n",
            "Train Loss: 0.0007\n",
            "Test Accuracy: 0.9091, Precision: 0.9128, Recall: 0.9091, F1: 0.9094, AUC: 0.9912\n",
            "Epoch 64/100\n",
            "Batch 0, Loss: 0.0000\n",
            "Batch 10, Loss: 0.0000\n",
            "Train Loss: 0.0001\n",
            "Test Accuracy: 0.9156, Precision: 0.9191, Recall: 0.9156, F1: 0.9160, AUC: 0.9911\n",
            "Epoch 65/100\n",
            "Batch 0, Loss: 0.0000\n",
            "Batch 10, Loss: 0.0000\n",
            "Train Loss: 0.0037\n",
            "Test Accuracy: 0.9156, Precision: 0.9205, Recall: 0.9156, F1: 0.9156, AUC: 0.9914\n",
            "Epoch 66/100\n",
            "Batch 0, Loss: 0.0003\n",
            "Batch 10, Loss: 0.0000\n",
            "Train Loss: 0.0004\n",
            "Test Accuracy: 0.9156, Precision: 0.9205, Recall: 0.9156, F1: 0.9156, AUC: 0.9911\n",
            "Epoch 67/100\n",
            "Batch 0, Loss: 0.0000\n",
            "Batch 10, Loss: 0.0000\n",
            "Train Loss: 0.0107\n",
            "Test Accuracy: 0.9091, Precision: 0.9115, Recall: 0.9091, F1: 0.9089, AUC: 0.9906\n",
            "Epoch 68/100\n",
            "Batch 0, Loss: 0.0000\n",
            "Batch 10, Loss: 0.0000\n",
            "Train Loss: 0.0008\n",
            "Test Accuracy: 0.9091, Precision: 0.9115, Recall: 0.9091, F1: 0.9089, AUC: 0.9910\n",
            "Epoch 69/100\n",
            "Batch 0, Loss: 0.0000\n",
            "Batch 10, Loss: 0.0000\n",
            "Train Loss: 0.0002\n",
            "Test Accuracy: 0.9156, Precision: 0.9205, Recall: 0.9156, F1: 0.9156, AUC: 0.9908\n",
            "Epoch 70/100\n",
            "Batch 0, Loss: 0.0000\n",
            "Batch 10, Loss: 0.0000\n",
            "Train Loss: 0.0005\n",
            "Test Accuracy: 0.9156, Precision: 0.9205, Recall: 0.9156, F1: 0.9156, AUC: 0.9909\n",
            "Epoch 71/100\n",
            "Batch 0, Loss: 0.0000\n",
            "Batch 10, Loss: 0.0000\n",
            "Train Loss: 0.0026\n",
            "Test Accuracy: 0.9221, Precision: 0.9262, Recall: 0.9221, F1: 0.9209, AUC: 0.9904\n",
            "Epoch 72/100\n",
            "Batch 0, Loss: 0.2819\n",
            "Batch 10, Loss: 0.0115\n",
            "Train Loss: 0.0262\n",
            "Test Accuracy: 0.8636, Precision: 0.8743, Recall: 0.8636, F1: 0.8641, AUC: 0.9741\n",
            "Epoch 73/100\n",
            "Batch 0, Loss: 0.0060\n",
            "Batch 10, Loss: 0.0000\n",
            "Train Loss: 0.0364\n",
            "Test Accuracy: 0.8442, Precision: 0.8616, Recall: 0.8442, F1: 0.8472, AUC: 0.9624\n",
            "Epoch 74/100\n",
            "Batch 0, Loss: 0.0049\n",
            "Batch 10, Loss: 0.0760\n",
            "Train Loss: 0.0719\n",
            "Test Accuracy: 0.8182, Precision: 0.8509, Recall: 0.8182, F1: 0.8216, AUC: 0.9741\n",
            "Epoch 75/100\n",
            "Batch 0, Loss: 0.0204\n",
            "Batch 10, Loss: 0.0229\n",
            "Train Loss: 0.0486\n",
            "Test Accuracy: 0.7403, Precision: 0.7592, Recall: 0.7403, F1: 0.7176, AUC: 0.9320\n",
            "Epoch 76/100\n",
            "Batch 0, Loss: 0.1053\n",
            "Batch 10, Loss: 0.2461\n",
            "Train Loss: 0.0911\n",
            "Test Accuracy: 0.8506, Precision: 0.8553, Recall: 0.8506, F1: 0.8497, AUC: 0.9691\n",
            "Epoch 77/100\n",
            "Batch 0, Loss: 0.0070\n",
            "Batch 10, Loss: 0.0266\n",
            "Train Loss: 0.0515\n",
            "Test Accuracy: 0.7987, Precision: 0.8389, Recall: 0.7987, F1: 0.7926, AUC: 0.9564\n",
            "Epoch 78/100\n",
            "Batch 0, Loss: 0.0229\n",
            "Batch 10, Loss: 0.0239\n",
            "Train Loss: 0.0864\n",
            "Test Accuracy: 0.6883, Precision: 0.7526, Recall: 0.6883, F1: 0.6746, AUC: 0.9206\n",
            "Epoch 79/100\n",
            "Batch 0, Loss: 0.3084\n",
            "Batch 10, Loss: 0.0080\n",
            "Train Loss: 0.0509\n",
            "Test Accuracy: 0.8636, Precision: 0.8787, Recall: 0.8636, F1: 0.8649, AUC: 0.9719\n",
            "Epoch 80/100\n",
            "Batch 0, Loss: 0.0228\n",
            "Batch 10, Loss: 0.0469\n",
            "Train Loss: 0.0533\n",
            "Test Accuracy: 0.7078, Precision: 0.7794, Recall: 0.7078, F1: 0.6805, AUC: 0.9422\n",
            "Epoch 81/100\n",
            "Batch 0, Loss: 0.0284\n",
            "Batch 10, Loss: 0.0108\n",
            "Train Loss: 0.0478\n",
            "Test Accuracy: 0.8117, Precision: 0.8245, Recall: 0.8117, F1: 0.8126, AUC: 0.9663\n",
            "Epoch 82/100\n",
            "Batch 0, Loss: 0.0074\n",
            "Batch 10, Loss: 0.0057\n",
            "Train Loss: 0.0360\n",
            "Test Accuracy: 0.8506, Precision: 0.8639, Recall: 0.8506, F1: 0.8527, AUC: 0.9804\n",
            "Epoch 83/100\n",
            "Batch 0, Loss: 0.1057\n",
            "Batch 10, Loss: 0.0749\n",
            "Train Loss: 0.0246\n",
            "Test Accuracy: 0.8182, Precision: 0.8311, Recall: 0.8182, F1: 0.8189, AUC: 0.9685\n",
            "Epoch 84/100\n",
            "Batch 0, Loss: 0.0077\n",
            "Batch 10, Loss: 0.0004\n",
            "Train Loss: 0.0377\n",
            "Test Accuracy: 0.8831, Precision: 0.9133, Recall: 0.8831, F1: 0.8881, AUC: 0.9716\n",
            "Epoch 85/100\n",
            "Batch 0, Loss: 0.0198\n",
            "Batch 10, Loss: 0.0021\n",
            "Train Loss: 0.0224\n",
            "Test Accuracy: 0.8831, Precision: 0.9043, Recall: 0.8831, F1: 0.8860, AUC: 0.9713\n",
            "Epoch 86/100\n",
            "Batch 0, Loss: 0.0060\n",
            "Batch 10, Loss: 0.0128\n",
            "Train Loss: 0.0374\n",
            "Test Accuracy: 0.8442, Precision: 0.8662, Recall: 0.8442, F1: 0.8429, AUC: 0.9712\n",
            "Epoch 87/100\n",
            "Batch 0, Loss: 0.0040\n",
            "Batch 10, Loss: 0.0038\n",
            "Train Loss: 0.0378\n",
            "Test Accuracy: 0.8831, Precision: 0.8927, Recall: 0.8831, F1: 0.8828, AUC: 0.9753\n",
            "Epoch 88/100\n",
            "Batch 0, Loss: 0.0097\n",
            "Batch 10, Loss: 0.0470\n",
            "Train Loss: 0.0099\n",
            "Test Accuracy: 0.8896, Precision: 0.9028, Recall: 0.8896, F1: 0.8910, AUC: 0.9808\n",
            "Epoch 89/100\n",
            "Batch 0, Loss: 0.0011\n",
            "Batch 10, Loss: 0.0002\n",
            "Train Loss: 0.0333\n",
            "Test Accuracy: 0.8636, Precision: 0.8688, Recall: 0.8636, F1: 0.8652, AUC: 0.9753\n",
            "Epoch 90/100\n",
            "Batch 0, Loss: 0.0520\n",
            "Batch 10, Loss: 0.0602\n",
            "Train Loss: 0.0263\n",
            "Test Accuracy: 0.8636, Precision: 0.8714, Recall: 0.8636, F1: 0.8658, AUC: 0.9683\n",
            "Epoch 91/100\n",
            "Batch 0, Loss: 0.0000\n",
            "Batch 10, Loss: 0.0070\n",
            "Train Loss: 0.0054\n",
            "Test Accuracy: 0.8961, Precision: 0.9241, Recall: 0.8961, F1: 0.8989, AUC: 0.9908\n",
            "Epoch 92/100\n",
            "Batch 0, Loss: 0.0002\n",
            "Batch 10, Loss: 0.0017\n",
            "Train Loss: 0.0461\n",
            "Test Accuracy: 0.8701, Precision: 0.8712, Recall: 0.8701, F1: 0.8705, AUC: 0.9708\n",
            "Epoch 93/100\n",
            "Batch 0, Loss: 0.1057\n",
            "Batch 10, Loss: 0.0021\n",
            "Train Loss: 0.0317\n",
            "Test Accuracy: 0.8571, Precision: 0.8576, Recall: 0.8571, F1: 0.8494, AUC: 0.9771\n",
            "Epoch 94/100\n",
            "Batch 0, Loss: 0.0006\n",
            "Batch 10, Loss: 0.0028\n",
            "Train Loss: 0.0165\n",
            "Test Accuracy: 0.9091, Precision: 0.9133, Recall: 0.9091, F1: 0.9099, AUC: 0.9686\n",
            "Epoch 95/100\n",
            "Batch 0, Loss: 0.0062\n",
            "Batch 10, Loss: 0.0009\n",
            "Train Loss: 0.0038\n",
            "Test Accuracy: 0.9091, Precision: 0.9099, Recall: 0.9091, F1: 0.9093, AUC: 0.9792\n",
            "Epoch 96/100\n",
            "Batch 0, Loss: 0.0001\n",
            "Batch 10, Loss: 0.0000\n",
            "Train Loss: 0.0090\n",
            "Test Accuracy: 0.8961, Precision: 0.9021, Recall: 0.8961, F1: 0.8963, AUC: 0.9799\n",
            "Epoch 97/100\n",
            "Batch 0, Loss: 0.0000\n",
            "Batch 10, Loss: 0.0025\n",
            "Train Loss: 0.0008\n",
            "Test Accuracy: 0.8831, Precision: 0.8846, Recall: 0.8831, F1: 0.8814, AUC: 0.9800\n",
            "Epoch 98/100\n",
            "Batch 0, Loss: 0.0011\n",
            "Batch 10, Loss: 0.0001\n",
            "Train Loss: 0.0056\n",
            "Test Accuracy: 0.9026, Precision: 0.9165, Recall: 0.9026, F1: 0.9053, AUC: 0.9796\n",
            "Epoch 99/100\n",
            "Batch 0, Loss: 0.0000\n",
            "Batch 10, Loss: 0.0008\n",
            "Train Loss: 0.0086\n",
            "Test Accuracy: 0.9026, Precision: 0.9149, Recall: 0.9026, F1: 0.9042, AUC: 0.9772\n",
            "Epoch 100/100\n",
            "Batch 0, Loss: 0.0001\n",
            "Batch 10, Loss: 0.0009\n",
            "Train Loss: 0.0076\n",
            "Test Accuracy: 0.9026, Precision: 0.9079, Recall: 0.9026, F1: 0.9024, AUC: 0.9747\n",
            "Training completed.\n",
            "Final Results on Original Dataset:\n",
            "Accuracy: 0.9351\n",
            "Precision: 0.9353\n",
            "Recall: 0.9351\n",
            "F1-score: 0.9346\n",
            "AUC: 0.9748\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase2: Augmentation"
      ],
      "metadata": {
        "id": "l8lxrCNt8D96"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import albumentations as A\n",
        "from tqdm import tqdm\n",
        "\n",
        "def augment_and_save(input_dir, output_dir, num_augmentations=11):\n",
        "    # Define augmentations similar to the paper\n",
        "    aug_transform = A.Compose([\n",
        "        A.RandomRotate90(p=0.5),\n",
        "        A.Rotate(limit=45, p=0.5),\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.VerticalFlip(p=0.5),\n",
        "        A.RandomScale(scale_limit=(0.8, 1.25), p=0.5),\n",
        "        A.ShiftScaleRotate(shift_limit=0.3, scale_limit=0.3, rotate_limit=45, p=0.5),\n",
        "        A.RandomBrightnessContrast(brightness_limit=(0.1, 2), contrast_limit=(0.5, 2), p=0.5),\n",
        "        A.HueSaturationValue(hue_shift_limit=0.5, sat_shift_limit=(0.2, 3), val_shift_limit=0, p=0.5),\n",
        "    ])\n",
        "\n",
        "    class_counts = {}\n",
        "\n",
        "    for class_name in os.listdir(input_dir):\n",
        "        class_dir = os.path.join(input_dir, class_name)\n",
        "        output_class_dir = os.path.join(output_dir, class_name)\n",
        "        os.makedirs(output_class_dir, exist_ok=True)\n",
        "\n",
        "        class_counts[class_name] = 0\n",
        "\n",
        "        for img_name in tqdm(os.listdir(class_dir), desc=f\"Augmenting {class_name}\"):\n",
        "            img_path = os.path.join(class_dir, img_name)\n",
        "            image = np.array(Image.open(img_path).convert('RGB'))\n",
        "\n",
        "            # Save original image\n",
        "            output_path = os.path.join(output_class_dir, img_name)\n",
        "            Image.fromarray(image).save(output_path)\n",
        "            class_counts[class_name] += 1\n",
        "\n",
        "            # Generate augmented images\n",
        "            for i in range(num_augmentations):\n",
        "                aug_image = aug_transform(image=image)['image']\n",
        "                aug_name = f\"{os.path.splitext(img_name)[0]}_aug_{i}{os.path.splitext(img_name)[1]}\"\n",
        "                output_path = os.path.join(output_class_dir, aug_name)\n",
        "                Image.fromarray(aug_image).save(output_path)\n",
        "                class_counts[class_name] += 1\n",
        "\n",
        "    print(\"\\nAugmentation completed. Class distribution:\")\n",
        "    for class_name, count in class_counts.items():\n",
        "        print(f\"{class_name}: {count}\")\n",
        "\n",
        "    return class_counts\n",
        "\n",
        "def create_csv(augmented_dir, output_csv):\n",
        "    data = []\n",
        "    for class_name in os.listdir(augmented_dir):\n",
        "        class_dir = os.path.join(augmented_dir, class_name)\n",
        "        for img_name in os.listdir(class_dir):\n",
        "            img_path = os.path.join(class_name, img_name)\n",
        "            data.append([img_path, class_name])\n",
        "\n",
        "    df = pd.DataFrame(data, columns=['image_path', 'label'])\n",
        "    df = df.sample(frac=1).reset_index(drop=True)  # Shuffle the dataset\n",
        "    df.to_csv(output_csv, index=False)\n",
        "    print(f\"\\nCSV file created: {output_csv}\")\n",
        "    print(f\"Total samples: {len(df)}\")\n",
        "\n",
        "# Usage\n",
        "input_dir = '/content/Monkeypox Skin Image Dataset'\n",
        "output_dir = '/content/MSID_dataset/augmented_train'\n",
        "output_csv = '/content/MSID_dataset/augmented_train.csv'\n",
        "\n",
        "class_counts = augment_and_save(input_dir, output_dir)\n",
        "create_csv(output_dir, output_csv)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwpaOU8-4oN8",
        "outputId": "ae5a2521-5ea9-46e0-8f16-f2b1e35701e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Augmenting Measles: 100%|██████████| 91/91 [00:38<00:00,  2.37it/s]\n",
            "Augmenting Chickenpox: 100%|██████████| 107/107 [00:41<00:00,  2.60it/s]\n",
            "Augmenting Normal: 100%|██████████| 293/293 [01:36<00:00,  3.04it/s]\n",
            "Augmenting Monkeypox: 100%|██████████| 279/279 [01:55<00:00,  2.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Augmentation completed. Class distribution:\n",
            "Measles: 1092\n",
            "Chickenpox: 1284\n",
            "Normal: 3516\n",
            "Monkeypox: 3348\n",
            "\n",
            "CSV file created: /content/MSID_dataset/augmented_train.csv\n",
            "Total samples: 9240\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Augmented Data"
      ],
      "metadata": {
        "id": "SJnIzaH08M2_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "class MonkeypoxDataset(Dataset):\n",
        "    def __init__(self, csv_file, root_dir, transform=None):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.class_to_idx = {'Normal': 0, 'Monkeypox': 1, 'Chickenpox': 2, 'Measles': 3}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.root_dir, self.data.iloc[idx, 0])\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        label = self.class_to_idx[self.data.iloc[idx, 1]]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "class CustomModel(nn.Module):\n",
        "    def __init__(self, num_classes=4):\n",
        "        super(CustomModel, self).__init__()\n",
        "        self.efficientnet = models.efficientnet_v2_s(weights=models.EfficientNet_V2_S_Weights.IMAGENET1K_V1)\n",
        "\n",
        "        # Get the number of features from the last layer of EfficientNetV2\n",
        "        num_features = self.efficientnet.classifier[1].in_features\n",
        "\n",
        "        # Remove the classifier\n",
        "        self.efficientnet = nn.Sequential(*list(self.efficientnet.children())[:-1])\n",
        "\n",
        "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc1 = nn.Linear(num_features, 512)\n",
        "        self.dropout1 = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.dropout2 = nn.Dropout(0.3)\n",
        "        self.fc3 = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.efficientnet(x)\n",
        "        x = self.gap(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Data transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Create datasets and dataloaders\n",
        "train_dataset = MonkeypoxDataset('/content/MSID_dataset/augmented_train.csv', '/content/MSID_dataset/augmented_train', transform=transform)\n",
        "test_dataset = MonkeypoxDataset('test_original.csv', '/content/Monkeypox Skin Image Dataset', transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "print(f\"Training set size: {len(train_dataset)}\")\n",
        "print(f\"Test set size: {len(test_dataset)}\")\n",
        "\n",
        "# Initialize the model\n",
        "model = CustomModel()\n",
        "print(f\"Model initialized with {sum(p.numel() for p in model.parameters())} parameters\")\n",
        "\n",
        "# Move model to device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer and learning rate scheduler\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
        "\n",
        "# Training function\n",
        "def train(model, train_loader, criterion, optimizer, scheduler, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "        if i % 100 == 99:  # Print every 100 mini-batches\n",
        "            print(f'Batch {i+1}, Loss: {loss.item():.4f}')\n",
        "\n",
        "    scheduler.step()\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    return epoch_loss\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, test_loader, device):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            probs = F.softmax(outputs, dim=1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
        "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
        "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "    auc = roc_auc_score(all_labels, np.array(all_probs), multi_class='ovr', average='weighted')\n",
        "\n",
        "    return accuracy, precision, recall, f1, auc\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 100\n",
        "best_accuracy = 0.0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    train_loss = train(model, train_loader, criterion, optimizer, scheduler, device)\n",
        "    accuracy, precision, recall, f1, auc = evaluate(model, test_loader, device)\n",
        "\n",
        "    print(f\"Train Loss: {train_loss:.4f}\")\n",
        "    print(f\"Test Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}, AUC: {auc:.4f}\")\n",
        "\n",
        "    if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        torch.save(model.state_dict(), 'best_model_augmented.pth')\n",
        "\n",
        "print(\"Training completed.\")\n",
        "\n",
        "# Load the best model and evaluate\n",
        "model.load_state_dict(torch.load('best_model_augmented.pth'))\n",
        "final_accuracy, final_precision, final_recall, final_f1, final_auc = evaluate(model, test_loader, device)\n",
        "\n",
        "print(\"Final Results on Augmented Dataset:\")\n",
        "print(f\"Accuracy: {final_accuracy:.4f}\")\n",
        "print(f\"Precision: {final_precision:.4f}\")\n",
        "print(f\"Recall: {final_recall:.4f}\")\n",
        "print(f\"F1-score: {final_f1:.4f}\")\n",
        "print(f\"AUC: {final_auc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "0WEuoIRS7U2g",
        "outputId": "a76f6e17-1427-4a7e-c994-9ce0d5d6cdf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/MSID_dataset/augmented_train.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-a52228e0d634>\u001b[0m in \u001b[0;36m<cell line: 68>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;31m# Create datasets and dataloaders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMonkeypoxDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/MSID_dataset/augmented_train.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/content/MSID_dataset/augmented_train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMonkeypoxDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test_original.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/content/Monkeypox Skin Image Dataset'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-a52228e0d634>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, csv_file, root_dir, transform)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mMonkeypoxDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsv_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroot_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1447\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1448\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1706\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    861\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    864\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/MSID_dataset/augmented_train.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GCn4b2xT7U5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ClnSjvI47U7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ensemble Model and Preparing for Hugging Face deploy\n"
      ],
      "metadata": {
        "id": "HhNLQmYjfkVP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class EnsembleModel(nn.Module):\n",
        "    def __init__(self, model1, model2):\n",
        "        super(EnsembleModel, self).__init__()\n",
        "        self.model1 = model1\n",
        "        self.model2 = model2\n",
        "\n",
        "    def forward(self, x):\n",
        "        out1 = self.model1(x)\n",
        "        out2 = self.model2(x)\n",
        "        return (out1 + out2) / 2  # Simple averaging of predictions\n",
        "\n",
        "# Assuming you have already trained model1 and model2\n",
        "ensemble_model = EnsembleModel(model1, model2)"
      ],
      "metadata": {
        "id": "ARRfz6LFftk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xl2jBxMzftiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-O1IK5Gvftft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qZ2PmYg6ftWL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}